# Configuration for Transformer Tracker Training

data:
  path: data/MOT20
  sequence_length: 10

model:
  d_model: 512
  nhead: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  dim_feedforward: 2048
  dropout: 0.1

training:
  epochs: 100
  batch_size: 8
  learning_rate: 0.0001
  weight_decay: 0.0001
  min_lr: 1e-6
  lambda_match: 2.0
  lambda_bbox: 1.0
  lambda_state: 0.5
  num_workers: 4


